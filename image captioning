import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.models import ResNet50_Weights
from collections import Counter
from PIL import Image
import random
from tqdm import tqdm

# Set seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)

# Hyperparameters and constants
EMBED_DIM = 256
HIDDEN_DIM = 512
LEARNING_RATE = 0.001
BATCH_SIZE = 64
EPOCHS = 10
MIN_WORD_FREQ = 5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_WORKERS = 2

# Dataset and captions path - change to your local paths
IMAGES_DIR = "flickr8k/Images"
CAPTIONS_FILE = "flickr8k/captions.txt"

# Vocabulary class to map words to integer indices and vice versa
class Vocabulary:
    def __init__(self, freq_threshold):
        self.freq_threshold = freq_threshold
        self.itos = {0: "pad", 1: "startofseq", 2: "endofseq", 3: "unk"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.index = 4

    def __len__(self):
        return len(self.itos)

    def tokenizer(self, text):
        text = text.lower()
        return re.findall(r"\w+", text)

    def build_vocabulary(self, sentence_list):
        frequencies = Counter()
        for sentence in sentence_list:
            tokens = self.tokenizer(sentence)
            frequencies.update(tokens)

        for word, freq in frequencies.items():
            if freq >= self.freq_threshold:
                self.stoi[word] = self.index
                self.itos[self.index] = word
                self.index += 1

    def numericalize(self, text):
        tokenized_text = self.tokenizer(text)
        return [
            self.stoi[token] if token in self.stoi else self.stoi["unk"]
            for token in tokenized_text
        ]

# Dataset class for Flicker8k or similar caption dataset
class ImageCaptionDataset(Dataset):
    def __init__(self, imgid2captions, vocab, transform=None):
        self.imgs_and_captions = []
        self.vocab = vocab
        self.transform = transform
        for img_id, captions in imgid2captions.items():
            for cap in captions:
                self.imgs_and_captions.append((img_id, cap))

    def __len__(self):
        return len(self.imgs_and_captions)

    def __getitem__(self, idx):
        img_id, caption = self.imgs_and_captions[idx]
        img_path = os.path.join(IMAGES_DIR, img_id)
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        # Numericalize caption with start and end tokens
        numericalized_caption = [self.vocab.stoi["startofseq"]]
        numericalized_caption += self.vocab.numericalize(caption)
        numericalized_caption.append(self.vocab.stoi["endofseq"])
        caption_tensor = torch.tensor(numericalized_caption, dtype=torch.long)
        return image, caption_tensor

# Custom collate function to pad captions to max length in batch
def collate_fn(batch):
    images = []
    captions = []

    for img, cap in batch:
        images.append(img)
        captions.append(cap)

    images = torch.stack(images)

    # Padding captions
    lengths = [len(cap) for cap in captions]
    max_len = max(lengths)
    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)
    padded_captions.fill_(0)  # pad with index 0 ('pad')

    for i, cap in enumerate(captions):
        end = lengths[i]
        padded_captions[i, :end] = cap[:end]

    lengths = torch.tensor(lengths, dtype=torch.long)

    return images, padded_captions, lengths

# Encoder using pretrained ResNet50 as feature extractor
class ResNetEncoder(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)
        # Freezing all except last layer for fine tuning
        for param in resnet.parameters():
            param.requires_grad = False
        modules = list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.fc = nn.Linear(resnet.fc.in_features, embed_dim)
        self.batchnorm = nn.BatchNorm1d(embed_dim, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)
        features = features.view(features.size(0), -1)
        features = self.fc(features)
        features = self.batchnorm(features)
        return features

# Decoder using LSTM
class DecoderLSTM(nn.Module):
    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers=1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.3)

    def forward(self, features, captions):
        captions_input = captions[:, :-1]  # Remove last token for input
        embeddings = self.embedding(captions_input)
        features = features.unsqueeze(1)
        inputs = torch.cat((features, embeddings), dim=1)
        lstm_out, _ = self.lstm(inputs)
        outputs = self.linear(self.dropout(lstm_out))
        return outputs

# The complete image-captioning model
class ImageCaptioningModel(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, images, captions):
        features = self.encoder(images)
        outputs = self.decoder(features, captions)
        return outputs

# Load captions from file and organize by image id
def load_captions(captions_file):
    imgid2captions = {}
    with open(captions_file, "r", encoding="utf-8") as f:
        next(f)  # Skip header if present
        for line in f:
            line = line.strip()
            if not line:
                continue
            img_caption_pair = line.split('\t')
            if len(img_caption_pair) != 2:
                continue
            img_id_with_num, caption = img_caption_pair
            img_id = img_id_with_num.split("#")[0]
            if img_id not in imgid2captions:
                imgid2captions[img_id] = []
            imgid2captions[img_id].append(caption)
    return imgid2captions

# Split dataset into train and val sets
def train_val_split(imgid2captions, val_ratio=0.1):
    all_img_ids = list(imgid2captions.keys())
    random.shuffle(all_img_ids)
    val_size = int(len(all_img_ids) * val_ratio)
    val_ids = all_img_ids[:val_size]
    train_ids = all_img_ids[val_size:]

    train_captions = {img_id: imgid2captions[img_id] for img_id in train_ids}
    val_captions = {img_id: imgid2captions[img_id] for img_id in val_ids}
    return train_captions, val_captions

# Training function
def train_one_epoch(model, dataloader, criterion, optimizer, vocab_size, epoch):
    model.train()
    total_loss = 0
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}", unit="batch")
    for images, captions, _lengths in progress_bar:
        images = images.to(DEVICE)
        captions = captions.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images, captions)
        # Shift outputs and captions for target labels
        outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size)
        targets = captions[:, 1:].contiguous().view(-1)

        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
    avg_loss = total_loss / len(dataloader)
    return avg_loss

# Validation function
def validate(model, dataloader, criterion, vocab_size):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for images, captions, _lengths in dataloader:
            images = images.to(DEVICE)
            captions = captions.to(DEVICE)
            outputs = model(images, captions)
            outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size)
            targets = captions[:, 1:].contiguous().view(-1)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
    avg_val_loss = total_loss / len(dataloader)
    return avg_val_loss

# Caption generation (inference) function
def generate_caption(model, image, vocab, max_length=20):
    model.eval()
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225])])
    image = transform(image).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        features = model.encoder(image)
    states = None
    caption_ids = [vocab.stoi["startofseq"]]
    for _ in range(max_length):
        cap_tensor = torch.tensor(caption_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)
        embeddings = model.decoder.embedding(cap_tensor)
        if len(caption_ids) == 1:
            inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)
        else:
            inputs = embeddings
        if states is None:
            lstm_out, states = model.decoder.lstm(inputs)
        else:
            lstm_out, states = model.decoder.lstm(embeddings[:, -1:].unsqueeze(1), states)
        output = model.decoder.linear(lstm_out.squeeze(1))
        _, predicted = output.max(1)
        next_word = predicted.item()
        caption_ids.append(next_word)
        if next_word == vocab.stoi["endofseq"]:
            break
    caption_words = [vocab.itos.get(idx, "unk") for idx in caption_ids[1:-1]]
    return " ".join(caption_words)

def main():
    if not os.path.exists(IMAGES_DIR) or not os.path.exists(CAPTIONS_FILE):
        print(f"Please make sure the paths exist:\nImages dir: {IMAGES_DIR}\nCaptions file: {CAPTIONS_FILE}")
        return

    # Load captions
    imgid2captions = load_captions(CAPTIONS_FILE)
    print(f"Loaded {len(imgid2captions)} images with captions.")

    # Build vocabulary
    all_captions = []
    for caps in imgid2captions.values():
        all_captions.extend(caps)

    vocab = Vocabulary(freq_threshold=MIN_WORD_FREQ)
    vocab.build_vocabulary(all_captions)
    print(f"Vocabulary size: {len(vocab)}")

    # Split dataset into train and val
    train_captions, val_captions = train_val_split(imgid2captions, val_ratio=0.1)
    print(f"Training images: {len(train_captions)} | Validation images: {len(val_captions)}")

    # Data transforms
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])

    # Datasets and dataloaders
    train_dataset = ImageCaptionDataset(train_captions, vocab, transform=train_transform)
    val_dataset = ImageCaptionDataset(val_captions, vocab, transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=NUM_WORKERS, collate_fn=collate_fn)

    # Initialize model
    encoder = ResNetEncoder(EMBED_DIM).to(DEVICE)
    decoder = DecoderLSTM(EMBED_DIM, HIDDEN_DIM, len(vocab)).to(DEVICE)
    model = ImageCaptioningModel(encoder, decoder).to(DEVICE)

    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi["pad"])
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    best_val_loss = float('inf')
    for epoch in range(EPOCHS):
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, len(vocab), epoch)
        val_loss = validate(model, val_loader, criterion, len(vocab))
        print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # Save best model checkpoint
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), "best_image_caption_model.pth")
            print(f"Saved best model at Epoch {epoch+1}")

    # Example inference
    print("\nRunning example caption generation...")
    example_image_path = random.choice(list(imgid2captions.keys()))
    example_image_full_path = os.path.join(IMAGES_DIR, example_image_path)
    image = Image.open(example_image_full_path).convert("RGB")
    caption = generate_caption(model, image, vocab)
    print(f"Generated caption for image {example_image_path}:")
    print(caption)

if __name__ == "__main__":
    main()

